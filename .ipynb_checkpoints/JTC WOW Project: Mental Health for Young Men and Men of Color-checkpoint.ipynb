{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mental Health for Young Men and Men of Color**#\n",
    "\n",
    "**By : Jeneil Stallion and Hernan Carvente-Martinez**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As behavioral health advocates who work in the public health field, our passion is to raise awareness on behavioral health in our community. Historically and currently, men of color—-particularly African-American and Latinx, are overburdened by unaddressed mental and behavioral health disparities; often this stems from adverse childhood experiences and/or structural violence leading into adulthood. Within the field of behavioral health, Black and Brown men are under-represented in this profession. It can be an asset to have health professionals within the care network who have cultural context and lived experience addressing trauma as men of color, so we decided to show the disparity of resources targeted towards young men and men of color.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project highlights the disparities of mental health resources specifically for young men and men of color. We chose to do research on NAMI (National Alliance on Mental Illness), and NIMH (National Institute of Mental Health), two of the top mental health organizations in the country. To accomplish this task, we used the following skills and resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python’s requests library\n",
    "- Beautiful soup to parse and web scrape the data\n",
    "- Created a For loop to iterate through specific ‘Keywords” and return search results\n",
    "- Defined functions to pull specific information from html documents and return data\n",
    "- Created a CSV file of parsed information from NAMI and NIMH websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIMH (National Institute of Mental Health) Web Scrape by : Jeneil Stallion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import BeautifulSoup, Pythons's requests library, and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define webpage url to get data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nimh.nih.gov/health/find-help/index.shtml#part_150431'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request a response to make sure the website is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out contents of website to view html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create soup object to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the domain's url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimh_domain = 'https://www.nimh.gov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function get_link_info(title) which pulls the title and information of the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-46-1cba29cec91a>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-1cba29cec91a>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    link_info = {'Link': url , 'Desription':link.text}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def get_link_info(title):\n",
    "    #define function\n",
    "\tdata = []\n",
    "    #create list to append data into\n",
    "\tfor link in soup.find('section', {\"data-cms-title\": title}).findAll('a'):\n",
    "    #create for loop that finds all of the 'section' and title: data-cms-title'\n",
    "    #'a' is an acnchor element that creates a hyperlink to webpages. url is left blank so link will be returned\n",
    "\t\turl = ''\n",
    "\t\tif 'http://' in link['href']:\n",
    "\t\t\turl = link['href']\n",
    "        #if statement to add or remove wrong https:// hyperlink \n",
    "\t\telif 'https://' in link['href']:\n",
    "           #else if https:// is true is the href link, then print out href \n",
    "\t\t\turl = link['href']\n",
    "\t\telse: \n",
    "            #or else add the nimh_domain name to the link\n",
    "\t\t\turl = f\"{nimh_domain}{link['href']}\"\n",
    "\t\tprint(link.text)\n",
    "\t\tparent = link.find_parent('p')\n",
    "    #find parent folder with paragraphs inside\n",
    "\t\tprint(parent.text)\n",
    "    #print out the text for parent folders\n",
    "\t\tfor child in soup.p.children:\n",
    "        #find the child folders of the parent folders to access contents\n",
    "\t\t\tprint(child)\n",
    "        #print child folder\n",
    "\t\t\tfor e in soup.findAll('br'):\n",
    "                #find all 'br' to identify breaks in code\n",
    "        #specify link info parsed 'Link Url' & 'Description' text\n",
    "\t\tlink_info = {'Link': url , 'Desription':link.text}\n",
    "\t\tdata.append(link_info)\n",
    "\treturn data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_healthcare_info(title):\n",
    "    #define function to get healthcare resources info\n",
    "\thealthcare_data = []\n",
    "    #create empty library to append data\n",
    "\tfor link in soup.find('section', {\"data-cms-title\": title}).findAll('a'):\n",
    "\t\turl = ''\n",
    "        #  #create for loop that finds all of the 'section' and title: data-cms-title'\n",
    "    #'a' is an acnchor element that creates a hyperlink to webpages. url is left blank so link will be returned\n",
    "\t\tif 'http://' in link['href']:\n",
    "\t\t\turl = link['href']\n",
    "             #if statement to add or remove wrong https:// hyperlink \n",
    "\t\telif 'https://' in link['href']:\n",
    "\t\t\turl = link['href']\n",
    "             #else if https:// is true is the href link, then print out href \n",
    "\t\telse: \n",
    "\t\t\turl = f\"{nimh_domain}{link['href']}\"\n",
    "            # #or else add the nimh_domain name to the link\n",
    "\t\tprint(link.text)\n",
    "        #print conents of link\n",
    "\t\tlink_info = {'Link': url , 'Desription':link.text}\n",
    "        #Define link info with parsed data\n",
    "\t\thealthcare_data.append(link_info)\n",
    "        #append data to link info\n",
    "\treturn healthcare_data\n",
    "        #return data to healtcare_data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-141b9e311ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_link_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Get Immediate Help in a Crisis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#use funtion to get link contents information and print out data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_healthcare_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Get Immediate Help in a Crisis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#use funtion to get link contents information and print out data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c1f6bb188350>\u001b[0m in \u001b[0;36mget_link_info\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_link_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data-cms-title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "get_link_info('Get Immediate Help in a Crisis')\n",
    "#use funtion to get link contents information and print out data\n",
    "get_healthcare_info('Get Immediate Help in a Crisis')\n",
    "#use funtion to get link contents information and print out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3e1bcae61632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_link_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Get Immediate Help in a Crisis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#create csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mHeaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Link'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Desription'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#name headers of csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdict_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_link_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Get Immediate Help in a Crisis'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c1f6bb188350>\u001b[0m in \u001b[0;36mget_link_info\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_link_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data-cms-title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "get_link_info('Get Immediate Help in a Crisis')\n",
    "#create csv file\n",
    "Headers = ['Link', 'Desription']\n",
    "#name headers of csv file\n",
    "dict_data = get_link_info('Get Immediate Help in a Crisis',)\n",
    "#define data to be added to csv file\n",
    "healthcare_data = get_healthcare_info('Find a Health Care Provider or Treatment')\n",
    "#define data to be added to csv file\n",
    "NIMH_data = dict_data + healthcare_data \n",
    "#Add data together as one\n",
    "csv_file = \"NIMH_data_final.csv\"\n",
    "#name csv file\n",
    "try:\n",
    "#try statement\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "#open csv and write as csv file\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=Headers)\n",
    "    #specify writer and name of headers\n",
    "        writer.writeheader()\n",
    "        for data in NIMH_data:\n",
    "            writer.writerow(data)\n",
    "    print('csv complete')\n",
    "#print when complete\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n",
    "    #print if error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**National Allience for Mental Illness(NAMI) Web Scrape by: Hernan Carvente-Martinez**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are keywords that we searched on the nami website. \n",
    "# For 'African American', the %20 signifies a space in the url line\n",
    "keywords = ['Black','African%20American', 'Latino', 'Latinx','Latina',\n",
    "\t\t\t'Indigenous']\n",
    "\n",
    "# The set() method below creates an empty set where unique search results\n",
    "# are being placed once the line of code is run\n",
    "unique_results = set()\n",
    "\n",
    "# The lines of code below we are accessing a csv file where the unique results\n",
    "# above will be housed once the for loop goes through all the search keywords\n",
    "with open('nami_list.csv', newline='', mode='w') as csvfile:\n",
    "\tfieldnames = ['title', 'summary', 'link']\n",
    "\tnami_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\tnami_writer.writeheader()\n",
    "\n",
    "\t# Each keyword brings up multiple result pages so the for loop below goes \n",
    "\t# through every page of each keyword.\n",
    "\tfor number in range(1,10):\n",
    "\t\t#The for loop below is requesting the nami search page and going \n",
    "\t\t# through each of the keywords listed in the keywords list above. \t\t\n",
    "\t\tfor keyword in keywords:\n",
    "\t\t\t# This line of code contains the url for the nami search page.\n",
    "\t\t\t# We changed the keywork and the page number using the for loops\n",
    "\t\t\t# above to go through all the results.\n",
    "\t\t\turl = f'https://www.nami.org/Search?searchtext={keyword}&searchmode=allwords&bygroup=&bytopic=&bytype=139-141-142-143-144-145-146-147-149-153&page={number}'\n",
    "\t\t\t# The line of code below requests the nami url in text format \n",
    "\t\t\t# so that we can read and view the search contents in terminal \n",
    "\t\t\tnami_info = requests.get(url).text\n",
    "\t\t\t# The line of code below takes the url request above and creates\n",
    "\t\t\t# a soup object to parse the data\n",
    "\t\t\tsoup = BeautifulSoup(nami_info, 'lxml')\n",
    "\t\t\t\n",
    "\t\t\t# This is an empty list to place the title of each resource\n",
    "\t\t\ttitles_list = []\n",
    "\t\t\t# This is an empty list to place the summary of each resource \n",
    "\t\t\tsummary_list = []\n",
    "\t\t\t# This is an empty list to palce the url of each resource\n",
    "\t\t\turl_list = []\n",
    "\t\t\t\n",
    "\t\t\t# The for loop below is finding all of the tags in the html code \n",
    "\t\t\t# with the specific \"class\" for all the titles. \n",
    "\t\t\tfor resource_title in soup.find_all('div', class_='search-resultsTitle'):\n",
    "\t\t\t\t# This line defines titles with the specific anchor element\n",
    "\t\t\t\t# in the div tag where the title exists.\n",
    "\t\t\t\ttitle = resource_title.a.text\n",
    "\t\t\t\t# This line of code takes the title and adds it to the empty \n",
    "\t\t\t\t# titles list above and uses the strip function to remove\n",
    "\t\t\t\t# all of the extra spacing.\n",
    "\t\t\t\ttitles_list.append(title.strip())\n",
    "\n",
    "\t\t\t# The for loop below is finding all of the tags in the html code \n",
    "\t\t\t# with the specific \"class\" for all the summaries.\n",
    "\t\t\tfor resource_summary in soup.find_all('div', class_='search-resultsSummary'):\n",
    "\t\t\t\t# This line defines summary with the specific anchor element\n",
    "\t\t\t\t# in the div tag where the summary exists.\n",
    "\t\t\t\tsummary = resource_summary.text\n",
    "\t\t\t\t# This line of code takes the summary and adds it to the empty \n",
    "\t\t\t\t# summaries list above and uses the strip function to remove\n",
    "\t\t\t\t# all of the extra spacing.\n",
    "\t\t\t\tsummary_list.append(summary.strip())\n",
    "\n",
    "\t\t\t# The for loop below is finding all of the tags in the html code \n",
    "\t\t\t# with the specific \"class\" for all the url's.\n",
    "\t\t\tfor resource_url in soup.find_all('div', class_='search-resultsRelURL'):\n",
    "\t\t\t\t# This line defines summary with the specific anchor element\n",
    "\t\t\t\t# in the div tag where the url exists.\n",
    "\t\t\t\tlink = resource_url.a.get('href')\n",
    "\t\t\t\t# This line of code takes the url and adds it to the empty \n",
    "\t\t\t\t# url's list above and uses the strip function to remove\n",
    "\t\t\t\t# all of the extra spacing.\n",
    "\t\t\t\turl_list.append(link.strip())\n",
    "\n",
    "\t\t\t# The line below loops through each index in the lists that were \n",
    "\t\t\t# created. With each list being the same the same length.\n",
    "\t\t\tfor l in range(len(titles_list)):\n",
    "\t\t\t\t# This line of code adds each individual title with the\n",
    "\t\t\t\t# corresponding summary and url into the unique results empty\n",
    "\t\t\t\t# set created above.\t\t\n",
    "\t\t\t\tunique_results.add((titles_list[l], summary_list[l], url_list[l]))\t\t\n",
    "\n",
    "\t#This for loop goes through each result in the unique results set\n",
    "\tfor result in unique_results:\n",
    "\t\t# The line of code below adds each unique result to the CSV file we \n",
    "\t\t# created to house the results of the webscraping\t\n",
    "\t\tnami_writer.writerow({'title': result[0] , 'summary': result[1], 'link': result[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
